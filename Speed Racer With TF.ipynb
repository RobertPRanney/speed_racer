{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "# IMPORT STATEMENTS\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tf_agents.utils import common\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import categorical_q_network\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "\n",
    "from speed_racer import utils\n",
    "from speed_racer.env import SpeedRacer\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Set-up\n",
    "env_name = \"Speed-Racer\" # Episode are ~200-1000\n",
    "num_iterations = 12000     \n",
    "\n",
    "log_interval = 200 # how often to record mae\n",
    "eval_interval = 6000 # how often to eval model\n",
    "reporting_interval = 2000 # how often to print progress to screen\n",
    "\n",
    "initial_collect_steps = 2000 \n",
    "collect_steps_per_iteration = 4\n",
    "replay_buffer_capacity = 100000  \n",
    "\n",
    "network_name = 'small'\n",
    "fc_layer_params = (100,)\n",
    "batch_size = 64 \n",
    "learning_rate = 1e-3  \n",
    "gamma = 0.99       # we want to encourage faster solving so this has to be <1\n",
    "num_atoms = 51\n",
    "min_q_value = -300  \n",
    "max_q_value = 300  \n",
    "n_step_update = 4\n",
    "\n",
    "num_eval_episodes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Speed-Racer-rl-12k-small-100-4n\n"
     ]
    }
   ],
   "source": [
    "agent_name = f\"{env_name}-rl-{num_iterations / 1000:.0f}k-{network_name}-{fc_layer_params[0]}-{n_step_update}n\"\n",
    "print(f\"Training {agent_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_py_env = SpeedRacer()\n",
    "eval_py_env = SpeedRacer()\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-117.017"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "utils.compute_avg_return(eval_env, random_policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "    time_step = environment.current_time_step()\n",
    "    action_step = policy.action(time_step)\n",
    "    next_time_step = environment.step(action_step.action)\n",
    "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "    # Add trajectory to the replay buffer\n",
    "    replay_buffer.add_batch(traj)\n",
    "\n",
    "for _ in range(initial_collect_steps):\n",
    "    collect_step(train_env, random_policy)\n",
    "\n",
    "\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 2,000\n",
      "Steps: 4,000\n",
      "Steps: 6,000\n",
      "Steps: 8,000\n",
      "Steps: 10,000\n",
      "Steps: 12,000\n"
     ]
    }
   ],
   "source": [
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = utils.compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [(0, avg_return)]\n",
    "losses = [(0, 0)]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "    for _ in range(collect_steps_per_iteration):\n",
    "        collect_step(train_env, agent.collect_policy)\n",
    "\n",
    "    # Sample a batch of data from the buffer and update the agent's network.\n",
    "    experience, unused_info = next(iterator)\n",
    "    train_loss = agent.train(experience)\n",
    "\n",
    "    step = agent.train_step_counter.numpy()\n",
    "\n",
    "    if step % log_interval == 0:\n",
    "        losses.append((step, train_loss.loss))\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = utils.compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "        returns.append((step, avg_return))\n",
    "\n",
    "    if step % reporting_interval == 0:\n",
    "        print(f\"Steps: {step:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'metrics/rewards/{agent_name}.pkl', 'wb') as out_pickle:\n",
    "    pickle.dump(returns, out_pickle)\n",
    "with open(f'metrics/losses/{agent_name}.pkl', 'wb') as out_pickle:\n",
    "    pickle.dump(losses, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/ranneyr/miniconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: policies/Speed-Racer-rl-12k-small-100-4n/assets\n"
     ]
    }
   ],
   "source": [
    "policy = agent.collect_policy\n",
    "saver = PolicySaver(policy, batch_size=None)\n",
    "saver.save(f'policies/{agent_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_policy = tf.saved_model.load(f'policies/{agent_name}')\n",
    "policy_state = saved_policy.get_initial_state(batch_size=3)\n",
    "utils.make_video('videos/first_train.mp4', 60, saved_policy, eval_env, eval_py_env)\n",
    "utils.embed_video('videos/first_train.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for agent_name in os.listdir('metrics/rewards'):\n",
    "    with open(f'metrics/rewards/{agent_name}', 'rb') as in_pickle:\n",
    "        rewards = pickle.load(in_pickle)\n",
    "    steps = [x[0] for x in rewards]\n",
    "    rewards = utils.smooth_list([x[1] for x in rewards])\n",
    "    ax.plot(steps, rewards, label=f'{agent_name}')\n",
    "ax.set_ylabel('Average Return')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylim((-2500, 400))\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for agent_name in os.listdir('metrics/losses'):\n",
    "    with open(f'metrics/losses/{agent_name}', 'rb') as in_pickle:\n",
    "        losses = pickle.load(in_pickle)\n",
    "    steps = [x[0] for x in losses]\n",
    "    losses = utils.smooth_list([x[1] for x in losses])\n",
    "    ax.plot(steps, losses, label=f'{agent_name}')\n",
    "#ax.set_ylim((2, 3.2))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
